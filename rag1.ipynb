{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 74aebb552ea7... 100% ▕████████████████▏  68 MB                         \n",
      "pulling ef1438627c47... 100% ▕████████████████▏  190 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6f85a640a97c... 100% ▕████████████████▏ 807 MB                         \n",
      "pulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 6c0b08d96525... 100% ▕████████████████▏   65 B                         \n",
      "pulling 60f68b1aefd0... 100% ▕████████████████▏  193 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "!ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ollama) (2.10.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2023.7.22)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: idna in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sezer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\sezer\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('feedback_data.json', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1/34 to the database\n",
      "Added chunk 2/34 to the database\n",
      "Added chunk 3/34 to the database\n",
      "Added chunk 4/34 to the database\n",
      "Added chunk 5/34 to the database\n",
      "Added chunk 6/34 to the database\n",
      "Added chunk 7/34 to the database\n",
      "Added chunk 8/34 to the database\n",
      "Added chunk 9/34 to the database\n",
      "Added chunk 10/34 to the database\n",
      "Added chunk 11/34 to the database\n",
      "Added chunk 12/34 to the database\n",
      "Added chunk 13/34 to the database\n",
      "Added chunk 14/34 to the database\n",
      "Added chunk 15/34 to the database\n",
      "Added chunk 16/34 to the database\n",
      "Added chunk 17/34 to the database\n",
      "Added chunk 18/34 to the database\n",
      "Added chunk 19/34 to the database\n",
      "Added chunk 20/34 to the database\n",
      "Added chunk 21/34 to the database\n",
      "Added chunk 22/34 to the database\n",
      "Added chunk 23/34 to the database\n",
      "Added chunk 24/34 to the database\n",
      "Added chunk 25/34 to the database\n",
      "Added chunk 26/34 to the database\n",
      "Added chunk 27/34 to the database\n",
      "Added chunk 28/34 to the database\n",
      "Added chunk 29/34 to the database\n",
      "Added chunk 30/34 to the database\n",
      "Added chunk 31/34 to the database\n",
      "Added chunk 32/34 to the database\n",
      "Added chunk 33/34 to the database\n",
      "Added chunk 34/34 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for chunk, embedding in VECTOR_DB:\n",
    "    similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((chunk, similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve function took 2.02 seconds\n",
      "Retrieved knowledge:\n",
      " - (similarity: 0.90) Sample chunk 1\n",
      " - (similarity: 0.80) Sample chunk 2\n",
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - Sample chunk 1\n",
      " - Sample chunk 2\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Mock retrieve function for testing purposes\n",
    "def retrieve(query):\n",
    "    # Simulate a delay for testing\n",
    "    time.sleep(2)\n",
    "    return [(\"Sample chunk 1\", 0.9), (\"Sample chunk 2\", 0.8)]\n",
    "\n",
    "def retrieve_with_timeout(query, timeout=10):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(retrieve, query)\n",
    "        try:\n",
    "            return future.result(timeout=timeout)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(f\"Retrieve function timed out after {timeout} seconds\")\n",
    "            return []\n",
    "\n",
    "# Use a simple predefined input for testing\n",
    "input_query = \"Give me summary how is the logistics at the moment?\"\n",
    "\n",
    "start_time = time.time()\n",
    "retrieved_knowledge = retrieve_with_timeout(input_query, timeout=10)  # Set your desired timeout here\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Retrieve function took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "formatted_chunks = '\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])\n",
    "result = f\"{formatted_chunks}\"\n",
    "\n",
    "instruction_prompt = f\"\"\"You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{formatted_chunks}\"\"\"\n",
    "\n",
    "print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "I'm happy to provide you with an update on the logistics situation.\n",
      "\n",
      "Based on my analysis of Sample Chunk 1 and Sample Chunk 2, it appears that some shipping carriers are experiencing disruptions due to various factors such as increased demand, staffing shortages, and supply chain issues. The exact impact is still unfolding, but overall, there are reports of delays and inconvenience for customers who rely on these services.\n",
      "\n",
      "For example, Sample Chunk 1 mentioned that some carriers were struggling with high volumes of packages, leading to congestion in their sorting facilities. Similarly, Sample Chunk 2 noted that transportation costs were rising due to inflationary pressures.\n",
      "\n",
      "It's worth noting that the situation is not uniform across all regions or types of carriers, and some may be experiencing more disruptions than others.\n",
      "\n",
      "Overall, it seems that logistics are facing a challenging time as carriers navigate these challenges, but it's too early to say when everything will return to normal."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
